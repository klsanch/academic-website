+++
title = "Deep neural network features predict perceptual sensitivity and cortical responses to visual textures"
date = 2019-04-01T00:00:00

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Akshay V. Jagadeesh", "Justin L. Gardner"]

# Publication type.
# Legend:
# 0 = Uncategorized
# 1 = Conference proceedings
# 2 = Journal
# 3 = Work in progress
# 4 = Technical report
# 5 = Book
# 6 = Book chapter
publication_types = ["1"]

# Publication name and optional abbreviated version.
publication = "In *Vision Sciences Society*"
publication_short = "In *VSS*"

# Abstract and optional shortened version.
abstract = "Textures with similar visual features, scrambled locally, can be obviously distinguishable when viewed directly, but metameric (i.e. perceptually indistinguishable) when viewed in the periphery (Freeman & Simoncelli, 2011). This suggests that the visual system pools complex features over small regions of space. Prior studies of texture perception have utilized textures synthesized by iteratively updating random noise images to match handcrafted features derived from a linear filter bank (Portilla & Simoncelli, 2001). Extending this approach, we generated textures by matching complex features extracted from various layers of the VGG-19 convolutional neural network (Gatys et al., 2011), pooled over uniform-sized subregions of a naturalistic image. We asked five human observers to distinguish original images from feature-matched textures, presented at 10 degrees eccentricity. Feature-matched textures were less distinguishable from original images when features from later layers (e.g. pool4) (F=346.42, p<0.001) or within smaller pooling regions  (F=68.59, p<0.001) were matched. We modeled behavioral performance as a function of the distance between the features of the original image and texture on each trial. Comparing 12 different observer models, we found that a model using pool4 features computed within 2-degree pooling regions best predicts human performance on held-out trials. Furthermore, to assess the neural basis of texture perception, we measured BOLD activity in the visual cortex as five human subjects viewed texture images during ten 6 minute runs. Following published procedures (Freeman et al., 2013), images were flashed at 5Hz within 9 second blocks alternating between textures and phase-scrambles. We found that sensitivity to pool2 and pool4 textures relative to phase-scrambled images emerges in V2 and increases from V2 to V3 to V4. In sum, these results suggest that both feature complexity and pooling region size contribute to visual metamerism and that cortical representations in areas V2-V4 may support these perceptual effects."

# Featured image thumbnail (optional)
image_preview = ""

# Is this a selected publication? (true/false)
selected = false

# Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter the filename (excluding '.md') of your project file in `content/project/`.
#projects = ["example-external-project"]

# Links (optional).
url_pdf = ""
url_preprint = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = ""

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
#url_custom = [{name = "Custom Link", url = "http://example.org"}]

# Does the content use math formatting?
math = true

# Does the content use source code highlighting?
highlight = true

# Featured image
# Place your image in the `static/img/` folder and reference its filename below, e.g. `image = "example.jpg"`.
[header]
image = "headers/bubbles-wide.jpg"
caption = "My caption :smile:"

+++

**Featured** in [Psychonomic Society](https://featuredcontent.psychonomic.org/fortifying-memory-after-encoding-internal-and-external-attention-and-visual-short-term-memory/): Fortifying memory after encoding: Internal and external attention and visual short-term memory
